{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "preliminary-violence",
   "metadata": {},
   "source": [
    "# Getting Data from the Web\n",
    "The internet is full of useful (as well as useless) information, so sometimes it might be very helpful to get data from it and process it locally.\n",
    "\n",
    "There are different ways to get data from the web, the most used are:\n",
    "- **Web Scraping**: download the html content from the page and then look from it to extract information\n",
    "- **Using Web API**: APIs ([Application Programming Interface](https://en.wikipedia.org/wiki/API)) is code that is meant to be called from other code instead of displayed visually to a user. We'll be looking at the most common type: [REST API](https://en.wikipedia.org/wiki/Representational_state_transfer)\n",
    "\n",
    "Often Web API need authentication, but ofter you can get a API key after a quick free signup.\n",
    "[Here's a non-exaustive list](https://github.com/public-apis/public-apis) of open APIs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "honey-while",
   "metadata": {},
   "source": [
    "## External Libraries\n",
    "We'll be using these two external libraries:\n",
    "- [requests](https://requests.readthedocs.io/en/master/): a more user-friendly alternative to the built-in library `urllib.request`\n",
    "- [beautifulsoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) a html parser, which is a type of software that builds a data structure from given inputs (usually of text kind).\n",
    "\n",
    "which you can install by runnin the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fabulous-sandwich",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install requests \n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's import from them to check they were installed correctly\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-breakfast",
   "metadata": {},
   "source": [
    "## Web Scraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prerequisite-watts",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.metaweather.com/21125/\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bizarre-handle",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "html_doc = r.text\n",
    "html_doc[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seventh-consumption",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "if you want you can try render the html within the notebook by using\n",
    "```python\n",
    "from IPython.core.display import HTML\n",
    "HTML(r.text)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-pennsylvania",
   "metadata": {},
   "source": [
    "We could try an do string manipulation (or using [regular expression syntax](https://www.w3schools.com/python/python_regex.asp)), but using a pre-built parser is usually less painful :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regional-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(html_doc, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advisory-personality",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "weather_data = []\n",
    "\n",
    "for item in soup.findAll(\"div\"):\n",
    "    if item.has_attr(\"data-date\"):\n",
    "        weather_date = item[\"data-date\"]\n",
    "        weather_description = item.find(\"span\").get_text()\n",
    "\n",
    "        weather_day_data = {\n",
    "            \"date\": weather_date,\n",
    "            \"description\": weather_description,\n",
    "        }\n",
    "\n",
    "        weather_data.append(weather_day_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tested-theology",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "secure-penguin",
   "metadata": {},
   "source": [
    "## Web API\n",
    "APIs are usually much more stable and nicer to work with, but you usually need to read through some documentation to learn what you can do and which particular urls (called \"endpoints\") you need to use.\n",
    "\n",
    "In our case, [MetaWeather API documentation](https://www.metaweather.com/api/) tells us that to get a similar output as our scraped data, we need to use the endpoint `/api/location/(woeid)/` where `woeid` is the identifier of the location we want the weather from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.metaweather.com/api/location/21125?api=hjagr0r3hg03hrghg0g3rah0\")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-plenty",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_weather_data = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boolean-unemployment",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "api_weather_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "posted-abuse",
   "metadata": {},
   "source": [
    "# Pandas to the rescue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aggregate-insider",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "under-deployment",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "list_of_dfs = pd.read_html(\"https://en.wikipedia.org/wiki/S%26P_500\")\n",
    "first_df = list_of_dfs[1].iloc[:-4]  # last four columns are statistics\n",
    "df = first_df.set_index(\"Year\")\n",
    "\n",
    "change_values_as_strings = (\n",
    "    df[\"Change in Index\"].str.replace(\"âˆ’\", \"-\").str.replace(\"%\", \"\")\n",
    ")  # cleaning up wierd characters\n",
    "change_values_as_numbers = pd.to_numeric(change_values_as_strings)\n",
    "\n",
    "change_values_as_numbers.plot(grid=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authentic-presentation",
   "metadata": {},
   "source": [
    "# HomeWork\n",
    "\n",
    "For a more flexible application we could get the `woeid` from a city name or lat/lon coordinates by using other provided endpoints: `/api/location/search/?query=(query)` and `/api/location/search/?lattlong=(latt),(long)`\n",
    "\n",
    "Create two functions:\n",
    "```python\n",
    "def get_woeid_from_city_name(city_name):\n",
    "    ...\n",
    "\n",
    "def get_woeid_from_latlon(lat, lon):\n",
    "    ...\n",
    "```\n",
    "\n",
    "which will return the `woeid` for the given input. Then try to combine this with the previous code to produce a function that gets the weather for the next days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confident-possibility",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rural-brother",
   "metadata": {},
   "source": [
    "### Possible solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fatty-printer",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _get_woeid_from_city_name(city_name):\n",
    "    json_data = requests.get(f\"https://www.metaweather.com/api/location/search/?query={city_name}\").json()\n",
    "    if not json_data:\n",
    "        raise ValueError(f\"No city found with name: {city_name}\")\n",
    "    first_result_woeid = json_data[0][\"woeid\"]\n",
    "    return first_result_woeid\n",
    "\n",
    "\n",
    "def _get_woeid_from_latlon(lat, lon):\n",
    "    json_data = requests.get(f\"https://www.metaweather.com/api/location/search/?lattlong={lat},{lon}\").json()\n",
    "    if not json_data:\n",
    "        raise ValueError(f\"No location found with latitude {lat} and longitude {lon}\")\n",
    "    first_result_woeid = json_data[0][\"woeid\"]\n",
    "    return first_result_woeid\n",
    "\n",
    "\n",
    "# which can be tested with\n",
    "_get_woeid_from_city_name(\"Glasgow\")\n",
    "_get_woeid_from_latlon(55.864200, -4.251800)\n",
    "\n",
    "\n",
    "def _get_weather_from_woeid(woeid):\n",
    "    json_data = requests.get(f\"https://www.metaweather.com/api/location/{woeid}\").json()\n",
    "    json_data[\"consolidated_weather\"]\n",
    "    return json_data\n",
    "\n",
    "\n",
    "def print_weather(city_name=None, lat=None, lon=None):\n",
    "    if city_name is None:\n",
    "        woeid = _get_woeid_from_latlon(lat, lon)\n",
    "    else:\n",
    "        woeid = _get_woeid_from_city_name(city_name)\n",
    "\n",
    "    weather_data = _get_weather_from_woeid(woeid)\n",
    "\n",
    "    for i in weather_data[\"consolidated_weather\"]:\n",
    "        print(i[\"applicable_date\"], i[\"weather_state_name\"])\n",
    "\n",
    "\n",
    "# which can be tested with\n",
    "print_weather(\"Glasgow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-header",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
